{
  "general": {
    "title": "Evaluación Digital ",
    "subtitle": "EXTRACCIÓN DE CONOCIMIENTO Y TOMA DE DECISIONES ESTRATÉGICAS",
    "professor": "MTI JESÚS E. ROMERO MORENO"
  },
  "section1": {
    "name": "Opción Múltiple",
    "weight": 50,
    "questions": [
      {
        "question": "Una empresa de retail quiere analizar las ventas de los últimos 5 años para identificar tendencias estacionales y optimizar su inventario. ¿Qué tipo de sistema sería más adecuado para este propósito analítico y por qué?",
        "options": [
          "Una base de datos OLTP, porque gestiona transacciones diarias rápidamente.",
          "Un Data Warehouse (OLAP), porque está diseñado para consultas complejas sobre datos históricos y orientados a temas.",
          "Un sistema de archivos local, porque es la forma más sencilla de almacenar reportes de ventas."
        ],
        "correct": 1,
        "explanation": "Un Data Warehouse está optimizado para el análisis de grandes volúmenes de datos históricos (variante en el tiempo) y organizados por temas (ventas, clientes), a diferencia de las bases de datos operativas (OLTP) que se centran en transacciones actuales."
      },
      {
        "question": "Netflix necesita procesar en tiempo real las interacciones de millones de usuarios (clics, pausas, búsquedas) para alimentar su sistema de recomendaciones. ¿Qué tecnología del ciclo de Big Data es fundamental en la etapa de 'Generación' para manejar este flujo constante de datos?",
        "options": [
          "Apache Kafka, para gestionar flujos de datos en tiempo real de manera eficiente y confiable.",
          "HDFS (Hadoop Distributed File System), para almacenar los datos una vez que han sido procesados.",
          "Tableau, para visualizar los dashboards de interacción de los usuarios."
        ],
        "correct": 0,
        "explanation": "Apache Kafka es la tecnología clave que Netflix y otras empresas utilizan para capturar y manejar flujos masivos de datos en tiempo real (streaming) desde su origen, siendo crucial en la etapa de generación y recolección del ciclo de Big Data."
      },
      {
        "question": "Un banco implementó un Data Warehouse siguiendo la filosofía de Kimball. ¿Cuál fue su estrategia más probable y qué beneficio inmediato buscaba obtener?",
        "options": [
          "Construir primero un gran almacén de datos centralizado para toda la empresa para asegurar una única fuente de la verdad.",
          "Crear pequeños 'data marts' por departamento (créditos, marketing) para entregar valor y resultados rápidos a áreas específicas.",
          "Enfocarse únicamente en la ingesta de datos no estructurados para análisis de redes sociales."
        ],
        "correct": 1,
        "explanation": "La filosofía de Kimball propone un enfoque 'de abajo hacia arriba', comenzando con data marts departamentales para ofrecer soluciones ágiles y valor de negocio rápidamente, para luego integrarlos."
      },
      {
        "question": "Una startup de IA está desarrollando un modelo de Machine Learning para predecir el abandono de clientes. Disponen de datos masivos (Big Data) pero son inconsistentes y de diversas fuentes. ¿Qué proceso es crucial para preparar estos datos antes de entrenar el modelo?",
        "options": [
          "El proceso de ETL (Extracción, Transformación y Carga) para limpiar, estructurar y consolidar los datos en un Data Warehouse.",
          "La implementación directa de algoritmos de IA sobre los datos crudos para que el modelo aprenda de la 'variedad'.",
          "Únicamente almacenar los datos en un sistema como HDFS sin procesarlos."
        ],
        "correct": 0,
        "explanation": "Los modelos de Machine Learning requieren datos de alta calidad. El proceso ETL es fundamental para transformar los datos crudos y masivos (Big Data) en un formato limpio, consistente y estructurado, a menudo cargándolos en un Data Warehouse que servirá como fuente fiable para el entrenamiento."
      },
      {
        "question": "Al diseñar un Data Warehouse para una cadena de supermercados, el equipo decide usar un 'Esquema de Estrella'. ¿Cómo se estructurarán los datos para analizar el monto total de ventas por producto, tienda y fecha?",
        "options": [
          "Una gran tabla única con toda la información mezclada para simplificar las consultas.",
          "Una tabla de hechos central con métricas (monto de venta) y tablas de dimensión separadas (producto, tienda, tiempo) que la rodean.",
          "Múltiples tablas de hechos que comparten algunas dimensiones, como en un esquema de constelación."
        ],
        "correct": 1,
        "explanation": "El esquema de estrella es el más intuitivo y rápido para consultas. Consiste en una tabla de hechos central (los números, las métricas) conectada directamente a varias tablas de dimensión que proveen el contexto (quién, qué, cuándo, dónde)."
      },
      {
        "question": "Una plataforma de e-commerce quiere entender por qué las ventas de una categoría de productos cayeron el mes pasado. ¿Qué tipo de análisis de datos deberían aplicar para descubrir las causas raíz de este evento?",
        "options": [
          "Análisis Predictivo, para pronosticar las ventas del próximo mes.",
          "Análisis Descriptivo, para resumir cuántas unidades se vendieron.",
          "Análisis Diagnóstico, para identificar los factores y relaciones que contribuyeron a la caída."
        ],
        "correct": 2,
        "explanation": "El análisis diagnóstico se enfoca específicamente en entender las causas de un evento pasado, buscando responder al 'porqué' sucedió algo, lo cual es exactamente el objetivo de la empresa en este escenario."
      },
      {
        "question": "Netflix utiliza su red de distribución de contenido (CDN) llamada 'Open Connect' con servidores (OCAs) ubicados estratégicamente en todo el mundo. ¿En qué etapa del ciclo de vida de Big Data es más relevante esta infraestructura y cuál es su objetivo principal?",
        "options": [
          "En la etapa de Análisis, para procesar los datos con Machine Learning.",
          "En la etapa de Almacenamiento y Gestión, para reducir la latencia y optimizar la entrega de contenido al usuario.",
          "En la etapa de Generación, para capturar las interacciones iniciales de los usuarios."
        ],
        "correct": 1,
        "explanation": "Open Connect y sus OCAs son parte de la estrategia de Almacenamiento de Netflix. Su función es acercar el contenido (almacenarlo) geográficamente a los usuarios para minimizar el tiempo de carga (latencia) y mejorar la experiencia de visualización."
      },
      {
        "question": "Una empresa está migrando su Data Warehouse a la nube y adopta un enfoque ELT en lugar del ETL tradicional. ¿Cuál es el cambio fundamental en su proceso?",
        "options": [
          "La transformación de los datos ahora se realiza antes de cargarlos, usando un servidor intermedio.",
          "Los datos se extraen y se cargan en crudo directamente en el Data Warehouse en la nube, y la transformación ocurre después utilizando la potencia de la nube.",
          "Se elimina por completo la etapa de transformación para agilizar el proceso."
        ],
        "correct": 1,
        "explanation": "El enfoque moderno ELT (Extraer, Cargar, Transformar), habilitado por la escalabilidad de la nube, invierte el orden. Los datos se cargan primero en su estado bruto y luego se transforman 'in situ' aprovechando el poder de cómputo de plataformas como BigQuery o Redshift."
      },
      {
        "question": "Un equipo de marketing utiliza una herramienta para identificar qué productos suelen comprarse juntos (ej. pañales y cerveza). ¿Qué técnica de minería de datos están aplicando?",
        "options": [
          "Análisis de Clustering, para agrupar clientes con comportamientos similares.",
          "Análisis de Asociación, para descubrir relaciones entre variables, como en el análisis de cesta de mercado.",
          "Análisis Predictivo, para estimar las ventas futuras de cada producto."
        ],
        "correct": 1,
        "explanation": "El análisis de asociación es la técnica específica diseñada para encontrar relaciones o patrones de co-ocurrencia entre elementos en grandes conjuntos de datos, siendo el 'análisis de cesta de mercado' su aplicación más clásica."
      },
      {
        "question": "Una compañía quiere asegurar que los datos en su Data Warehouse sean un registro histórico inmutable para análisis de tendencias a largo plazo. ¿Qué característica clave de un Data Warehouse, definida por Bill Inmon, garantiza esto?",
        "options": [
          "Integrado, porque combina datos de múltiples fuentes.",
          "Orientado a temas, porque organiza los datos por áreas de negocio.",
          "No volátil, porque una vez que los datos se cargan, no se modifican ni eliminan."
        ],
        "correct": 2,
        "explanation": "La característica 'no volátil' es fundamental, ya que asegura que la información histórica se conserve intacta, permitiendo comparaciones consistentes y análisis de tendencias fiables a lo largo del tiempo."
      },
      {
        "question": "Una empresa de logística analiza datos de sensores de su flota de camiones (ubicación, velocidad, consumo) para optimizar rutas en tiempo real. Esto representa un desafío de Big Data principalmente relacionado con:",
        "options": [
          "Volumen y Velocidad, por la gran cantidad de datos generados constantemente.",
          "Solo Variedad, porque los datos provienen de diferentes tipos de sensores.",
          "Solo Veracidad, asegurando que los datos de ubicación sean precisos."
        ],
        "correct": 0,
        "explanation": "Los datos de sensores IoT son un ejemplo clásico de Big Data donde el desafío principal es manejar el enorme Volumen de datos que llegan a una alta Velocidad, requiriendo tecnologías de procesamiento en tiempo real."
      },
      {
        "question": "¿Cuál es la relación fundamental entre un Data Warehouse y el entrenamiento de un modelo de Machine Learning para predecir ventas?",
        "options": [
          "El Data Warehouse se usa solo para almacenar los resultados del modelo de Machine Learning.",
          "El Data Warehouse actúa como una fuente de datos históricos, limpios y estructurados, esenciales para entrenar un modelo de ML preciso.",
          "No existe una relación directa; los modelos de ML siempre se entrenan con datos en tiempo real de Big Data."
        ],
        "correct": 1,
        "explanation": "Un Data Warehouse es una fuente ideal para el entrenamiento de modelos de ML, ya que provee grandes cantidades de datos históricos de alta calidad, previamente limpiados y organizados, lo cual es un requisito para que los algoritmos aprendan patrones fiables."
      },
      {
        "question": "Un analista de negocios necesita un reporte que cruce información de ventas con datos de inventario para entender cómo el stock afecta las ventas. Ambos procesos tienen sus propias métricas. ¿Qué esquema de Data Warehouse es más adecuado para este tipo de análisis complejo entre diferentes procesos?",
        "options": [
          "Esquema de Estrella, porque es el más simple y rápido para consultas de un solo proceso.",
          "Esquema de Copo de Nieve, porque normaliza las dimensiones para ahorrar espacio.",
          "Esquema de Constelación (o Galaxia), porque permite que múltiples tablas de hechos (ventas, inventario) compartan dimensiones comunes."
        ],
        "correct": 2,
        "explanation": "El esquema de constelación, también llamado 'fact constellation' o galaxia, está diseñado específicamente para escenarios donde se necesita analizar la relación entre múltiples procesos de negocio, utilizando varias tablas de hechos que comparten dimensiones."
      },
      {
        "question": "En el ciclo de vida del Big Data, ¿cuál es el objetivo principal de la etapa de 'Interpretación'?",
        "options": [
          "Utilizar herramientas como Tableau para crear gráficos y dashboards.",
          "Combinar el análisis técnico y la experiencia humana para tomar decisiones estratégicas basadas en los hallazgos.",
          "Escribir consultas SQL para procesar los datos almacenados en HDFS o Cassandra."
        ],
        "correct": 1,
        "explanation": "La interpretación es la etapa final y crucial donde el valor de los datos se materializa. No se trata solo de ver los datos (visualización) o procesarlos, sino de usar el juicio humano y el contexto del negocio para traducir los 'insights' en acciones y estrategias concretas."
      },
      {
        "question": "Si definimos la Inteligencia Artificial (IA) como la simulación de la inteligencia humana en máquinas, ¿dónde se ubica el Machine Learning (ML)?",
        "options": [
          "El ML es un campo completamente separado de la IA que se enfoca solo en datos.",
          "El ML es un enfoque o subcampo de la IA que permite a las máquinas aprender de los datos sin ser programadas explícitamente.",
          "La IA es una subcategoría del ML, enfocada en la creación de robots."
        ],
        "correct": 1,
        "explanation": "Machine Learning es una de las ramas más importantes de la Inteligencia Artificial. Es el enfoque que ha impulsado los avances recientes, al permitir que los sistemas aprendan patrones y realicen tareas a partir de datos."
      }
    ]
  },
  "section2": {
    "name": "Relación de Columnas",
    "weight": 20,
    "pairs": {
      "Data Warehouse": "Repositorio centralizado de datos integrados e históricos, optimizado para análisis y la toma de decisiones estratégicas.",
      "ETL": "Proceso para extraer datos de diversas fuentes, limpiarlos y estandarizarlos, y finalmente cargarlos en un almacén de datos.",
      "Esquema de Estrella": "Modelo dimensional donde una tabla central de métricas cuantitativas se conecta directamente a tablas descriptivas de contexto.",
      "Big Data": "Concepto que describe grandes volúmenes de datos (estructurados o no) caracterizados por su velocidad, variedad y volumen, que requieren técnicas avanzadas para su procesamiento.",
      "Apache Spark": "Framework de procesamiento distribuido utilizado por empresas como Netflix para análisis a gran escala y machine learning, tanto en lotes como en tiempo real.",
      "Minería de Datos": "Proceso de descubrir patrones y conocimiento útil de forma automática en grandes conjuntos de datos, como parte del proceso KDD.",
      "Data Mart": "Subconjunto de un Data Warehouse, enfocado en las necesidades analíticas de un área o departamento específico de la empresa.",
      "No Volátil": "Característica de un Data Warehouse que asegura que los datos, una vez registrados, no se alteran, garantizando un registro histórico consistente."
    }
  },
  "section3": {
    "name": "Juego del Ahorcado",
    "weight": 30,
    "words": [
      {
        "word": "KAFKA",
        "hint": "Tecnología de streaming usada por Netflix para manejar el flujo de interacciones de usuarios en tiempo real."
      },
      {
        "word": "HADOOP",
        "hint": "Framework de software de código abierto para almacenar y procesar grandes conjuntos de datos de forma distribuida."
      },
      {
        "word": "TABLEAU",
        "hint": "Herramienta de Business Intelligence y visualización de datos usada para crear dashboards interactivos."
      },
      {
        "word": "CASSANDRA",
        "hint": "Base de datos NoSQL distribuida utilizada para almacenar grandes cantidades de datos con alta disponibilidad."
      },
      {
        "word": "PREDICTIVO",
        "hint": "Tipo de análisis que utiliza datos históricos para pronosticar resultados futuros."
      },
      {
        "word": "DIMENSION",
        "hint": "En un modelo de estrella, es la tabla que proporciona el contexto (quién, qué, dónde, cuándo) a los hechos."
      },
      {
        "word": "KIMBALL",
        "hint": "Pionero del Data Warehousing que abogó por un enfoque 'de abajo hacia arriba' comenzando con data marts."
      },
      {
        "word": "OLAP",
        "hint": "Término que describe el procesamiento analítico en línea, característico de los Data Warehouses."
      },
      {
        "word": "INSIGHTS",
        "hint": "Conocimiento valioso y significativo que se extrae del análisis de datos para guiar decisiones."
      },
      {
        "word": "INTEGRACION",
        "hint": "Característica clave de un Data Warehouse que consiste en unificar datos de fuentes diversas en una vista coherente."
      }
    ]
  }
}